## Discussion

*Potential topics for the discussion.*
*This section still needs a fair amount of work.*

#### Performance across datasets

vg was, overall, the most accurate genotyper in our benchmarks.
The other methods were superior in a handful datasets and situations, primarily when genotyping deletions.
That vg generally had the best accuracy when restricting the comparison to presence/absence even in most of these cases is evidence that the performance shortfall can be attributed to the graph genotyping rather than mapping pipeline.
We hope to address these issues in a future release.
The quality of the input SV catalog also has an impact on relative performance.
The GiaB catalog is more curated and, specifically for deletions in non-repeat regions, Delly and BayesTyper were better at predicting genotypes compared to vg.
This might be because the breakpoint resolution for this type of SV in these regions is better in this dataset compared to the HGSVC dataset which was derived mostly from long-read sequencing.
Similarly, SMRT-SV2 performs better for deletions in the pseudo-diploid genome constructed from two high quality genome assemblies of CHM cell lines.
 
#### Providing a resource to be used by large-scale sequencing project

As a result of this study we provide a variation graph containing XX millions of SNVs and indels from the 1000 Genomes Project as well as XX thousands of SVs derived from long-read sequencing.
This variation graph could serve as a richer reference for large scale projects that use short-read sequencing.
For instance, more and more large-scale projects are sequencing the genomes of thousands or hundreds of thousands of individuals, e.g. the Pancancer Analysis of Whole Genomes[@url:https://dcc.icgc.org/pcawg], the Genomics England initiative[@url:https://www.genomicsengland.co.uk], and the TOPMed consortium[@url:https://www.nhlbiwgs.org/].
These large WGS studies will provide a deeper look into the mechanism of common diseases and, in some cases, will be used directly in a clinical setting.
Clinicians and researchers are eager to use these growing WGS resources to interrogate the importance of SVs in disease at a scale never achieved before, either to get a more complete picture of the genetic factors of a disease or to produce a more comprehensive clinical report.
As sequencing reaches the clinic, whole-genome sequencing will become routine for many patients.
Clinicians will rely on variant calling and interpretation for diagnosis and treatment.
For variant interpretation in particular, a comprehensive and unified characterization of the genomic variation will be extremely valuable. 

#### Simplified usage

Some methods require additional information or special VCF formatting [@doi:10.1101/558247].
SVTyper was designed to use VCFs created by Lumpy.
The genotyping module from Delly was implemented for variants found by its discovery module.
SMRT-SV2 required the VCF created specifically for it by Audano et al.[@tag:audano2019].
Nebula, a new k-mer-based genotyper, requires reads from a individual with the SV during k-mer selection[@doi:10.1101/566620].
In contrast, vg can take as input either explicit or symbolic VCFs, as well as assembly alignment.

#### Benefits of de novo assemblies

Our results suggest that constructing a graph from de novo assembly alignment is more representative of the sequencing reads and leads to better SV genotyping.
De novo assemblies for human are becoming more and more common, for example from optimized mate-pair libraries[@tag:denmark] or long-read sequencing[@tag:denmark].
For an optimal representation of the genomic variation, we expect the future graphs to include information from the alignment of numerous de novo assemblies.
Aligning assembled contigs to existing variation graphs, like to ones created from SVs catalogs, is still experimental but could generate a genome graph augmented with both existing variant databases and new high-quality assemblies.

#### Future improvements in vg

The vg toolkit is in active development.
Read mapping is an area of constant improvement, both in terms of computational efficiency and accuracy.
For example, haplotype information can be modeled in variation graphs and, in the future, assist read mapping and variant calling.
These upcoming developments will directly benefit SV genotyping with vg.

#### Limitations

Copy number variants (CNVs) are currently represented as deletions or insertions. 
For this reason duplications are represented as additional sequence rather than encoded as a loop in the graph.
While this is sufficient to represent single copy changes, such as deletions or single tandem duplications, CNVs with multi-copies states are not addressed by the current implementation.
The genotyping algorithm would need to be extended to model copy number in order to assess these variants.

Near-breakpoint resolution is necessary.
Simulations have shown that SV genotyping with vg is robust to errors up to 10 bp in breakpoint location.
Variants with higher uncertainty in the breakpoint location, for example discovered through read coverage analysis, cannot be added to the graph.

The genotyping evaluation of inversions is limited by the lack of existing gold-standards.
We showed that vg is capable of genotyping simple inversions from simulation or the few discovered in the SV catalog from Audano et al.[@tag:audano2019].
However, most inversions are complex and involve small insertions/deletions around their breakpoints[@doi:10.1186/s13059-017-1158-6].
While these complex variants are difficult to represent in the VCF format, we expect that de novo assembly alignment will help integrating these variants into the graph.

