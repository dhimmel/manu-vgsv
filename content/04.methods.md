## Methods

### Toil-vg

Toil-vg is a set of Python scripts for simplifying vg tasks such as graph construction, read mapping and SV genotyping.  It uses the Toil workflow engine [@tag:toil] to seamlessly run pipelines locally, on clusters or on the cloud.  All variation graph analysis in this report was done using toil-vg, with the exact commands available [here](https://github.com/glennhickey/hgsvc) [todo: get everything in one place with reasonable docs].  The principal toil-vg commands used are described below.

#### Toil-vg construct

Toil-vg construct automates graph construction and indexing following the best practices put forth by the vg community.
Graph construction is parallelized across different sequences from the reference fasta, and different whole-genome indexes are created side by side when possible.
Phasing information from the input VCF can be used when available to preserve haplotypes in the GCSA2 pruning step, as well as to extract haploid sequences to simulate from.

#### Toil-vg map

Toil-vg map splits the input reads into batches, maps each batch in parallel, then merges the result.

#### Toil-vg call

A simple though very general variant caller has been implemented as vg call.
Here it is used to genotype structural variants already present in the graph, but the same algorithm can also be used for smaller variants such as SNPS, as well as making de-novo calls.
The algorithm is as follows:

1. The average read support for each node and edge, adjusted for mapping and base quality, is computed. The graph can optionally be augmented to include new variation from the reads.
2. The graph is then decomposed into snarls.  Briefly, a snarl is a subgraph defined by two end nodes, where cutting the graph at these nodes disconnects the snarl from the rest of the graph.  (todo: work on way to define this without getting into bidrected graphs, or is that a lost cause?). Snarls can be nested inside other snarls, and this nesting hierarchy forms a forest (todo: I don't think chains get used anywhere in vg call so we can ignore here). The snarl decomposition is a fundamental structure for identifying variants in a graph and are formally defined, along with an algorithm to identify them, in [@tag:snarls].
3. Root-level snarls from the decomposition are considered independently and in parallel.  Only snarls whose two ends lie on a reference (ie chromosome) path are considered as the VCF format used for ouptut requires reference positions.  The following steps are performed on each root snarl. 
4. A set of paths between the snarls end nodes are computed. (todo, consult with Adam about writing up the RepresentativeTraversalFinder)
5. The paths are ranked according to their average support.
6. A genotype is determined using the relative support of the best traversals, as well as the background read depth.
7. The VCF variants are derived from the paths.
(todo: expand and clarify these last steps.  could leave them fairly brief here and go into detail in the supplement).


Due to the high memory requirements of the current implementation of vg call, toil-vg call splits the input graph into 2.5Mb overlapping chunks along the reference path.
Each chunk is called independently in parallel and the results are concatenated into the output VCF.   

#### Toil-vg sveval

The variants are first normalized with `bcftools norm` to ensure consistent representation between called variants and baseline variants.
We then implemented an overlap-based strategy to compare SVs and compute evaluation metrics (sveval R package: [https://github.com/jmonlong/sveval](https://github.com/jmonlong/sveval)).

For deletions and inversions, the affected region in the reference genome is overlapped and matched between the two sets od SVs.
First, we select pairs of SVs with at least 10% reciprocal overlap.
Then for each variant we compute the proportion of its region that is covered by an overlapping variant in the other set.
If this coverage proportion is higher than 80%, the variant is considered *covered*.
True positives are covered variants from the call set or the truth set.
False positives are variants from the call set that are not covered.
False negative are variants from the truth set that are not covered.

For insertions, we select pairs of insertions that are located no farther than XX bp from each other.
We then align the inserted sequences using a Smith-Waterman alignment.
Then for each insertion we compute the proportion of its inserted sequence that aligns a matched variant in the other set.
As for deletions/inversions, this coverage proportion is used to annotate variants as true positives, false positives and false negatives.

sveval accepts VCF files with symbolic or explicit representation of the SVs.
If the explicit representation is used, multi-allelic variants are split and their sequences right-trimmed.
When inversions are considered, the reverse-complement of the ALT sequence of variants larger than 10 bp is aligned with the REF sequence and classified as an inversion if more than 80% of the sequence aligns.

We assess either the calling performance (absence/presence of a SV) or the genotyping performance.
For the *calling* evaluation, both heterozygous and homozygous alternate SVs are compared using the approach described above.
To compute genotype-level metrics, the heterozygous and homozygous SVs are compared separately.
Before splitting the variants by genotype, consecutive heterozygous variants are first stitched together if located at less that 20 bp from each other, and then merged into homozygous variants if their reciprocal overlap is at least 80%.

### Simulation experiment

We simulated a synthetic genome with SVs.
The genomic sequence was simulated and each SV separated by XX bp from each other. 
XXXX Insertions, deletions and inversions were added. 
The size of deletions and insertions followed the distribution of real SVs from the HGSVC catalog.
We used the same size distribution as deletions for inversions.
Then a VCF file for three simulated samples was produced with random genotypes (homozygous reference, heterozygous, homozygous alternate).
We created another VCF file containing errors in the SV definition.
The one or both breakpoints of deletions were shifted between 1 and 10 bp.
The location and sequence of insertions were also modified, either shifted or deleted at the flanks, again up to 10 bp. 
The different methods were tested using as input either the true VCF or the VCF that contained errors.
For vg, a graph was constructed from the VCF file, indexed and used to map simulated reads and call variants using toil-vg (see above/below/LINK).
Other methods take a VCF as input. COMMANDS

Paired-end reads were simulated using `vg sim` on graph that contained the true SVs.
Different read depth were tested: 1x, 3x, 7x, 10x, 13x, 20x.
Real Illumina reads from XX (see Glenn's simulation) were used to model base qualities and sequencing errors.

The genotypes called in each experiment (method/VCF with or without errors/sequencing depth) were compared to the true SV genotypes to compute the precision, recall and F1 score (see above/LINK).

#### Breakpoint fine-tuning using graph augmentation

vg can call variants after augmenting the graph with the read alignments to discover new variants (see toil-vg methods maybe).
We tested if this approach could fine-tune the breakpoint location of SVs in the graph.
We started with the graph that contained approximate SVs (1-10 bp errors in breakpoint location) and 20x simulated reads from the simulation experiment (see above/LINK).
The variants called after graph augmentation were compared with the true SVs and considered fine-tuned if the breakpoints matched exactly.


### HGSVC Analysis

Phased VCFs were obtained for the three HGSVC samples from the authors of [@tag:hgsvc] and combined with bcftools merge.
A variation graph was created and indexed using the combined VCF and the HS38D1 reference with alt loci excluded.
The phasing information was used to construct a GWBT index, from which the two haploid sequences from HG00514 were extracted.
Illumina read pairs with 30x coverage were simulated from these sequences using vg, with an error model learned from real reads from the same sample.
Still, these reads reflect the idealized situation where the breakpoints of the SVs being genotyped are exactly known a priori.
The reads were mapped to the graph and the mappings used to genotype the SVs in the graph, which were finally compared back to the HG00514 genotypes from the HGSVC VCF.
The process was repeated with the same reads on the linear reference, using bwa-mem for mapping and DELLY, SVTYPER and Bayestyper for SV genotyping.

Illumina HiSeq 2500 paired end reads were downloaded from the EBI's ENA FTP site for the three samples, using Run Accessions ERR903030, ERR895347 and ERR894724 for HG00514, HG00733 and NA19240, respectively.
The graph and linear mapping and genotyping pipelines were run exactly as for the simulation, and the comparison results were aggregated across the three samples.

### GIAB Analysis

Version 0.6 of the GIAB SV VCF for the Ashkenazim son (HG002) was obtained from the NCBI FTP site.
Illumina reads downsampled to 50x coverage obtained as described in [@tag:vgnbt], were used to run the vg and linear SV genotyping pipelines described above though with GRCh37 instead of 38.
Since this dataset contains only one sample, variants without a determined genotype (14649 out of 74012) were considered "false positives" as a proxy measure for precision.

Todo: I think some more thought may need to go into this comparison.  I'd actually assumed before writing this up that the whole trio was in the VCF but this is not the case -- it's just for the one sample.  The curreny results will count as a false positive a call that was not assigned a genotype by GIAB.  This is probably a pretty good estimate, but I think it's too hand wavy to publish.  In the worst case, we can switch to just looking at recall on the whole set (which has the advantage of being exactly what that other paper did for deletions).

### SMRT-SV2 Comparison (CHMPD and SVPOP)

The SMRT-SV2 genotyper can only be used to genotype VCFs that were created by SMRT-SV2, and therefore could not be run on our simulated, HGSVC or GIAB data.
The authors shared their training and evaluation set, a pseudodiploid sample constructed from combining the haploid CHM1 and CHM13 samples, along with a negative control (NA19240).  
The high quality of the CHM assemblies makes this set an attractive alternative to using simulated reads.
We used this two-sample pseudodiploid VCF along with the 30X read set to construct, map and genotype with vg, and also ran SMRT-SV2 genotyper with the "30x-4" model and min-call-depth 8 cutoff, and compared the two back to the original VCF.

In an effort to extend this comparison to a more realistic setting, we reran the three HGSVC samples against the SMRT-SV2 discovery VCF (which contains them in addition to 12 other samples) published in [@tag:audano2019] using vg and SMRT-SV2 Genotyper.
The discovery VCF does not contain genotypes so we did not distinguish between heterozygous and homozygous genotypes, looking at only the presence or absence of an alt allele in each variant.


