## Results

### Structural variation in vg

We used vg to implement a straightforward SV genotyping pipeline.
Reads are mapped to the graph and used to compute the read support for each node and edge (see [Supplementary Information](#supplementary-information) for a description of the graph formalism).
Sites of variation are then identified using the snarl (aka "bubble") decomposition as described in [@tag:snarls], each resulting site being represented as a subgraph of the larger graph.
For each site, we determine the two most supported paths (haplotypes), and use their relative support in the read evidence to produce a genotype at that site (Figure {@fig:1}a).
We describe the pipeline in more detail in [Methods](#toil-vg).
We rigorously evaluated the accuracy of our method on a variety of datasets, and present these results in the remainder of this section.

![**Structural variation in vg.** 
a) vg uses the read coverage over possible paths to genotype variants in a bubble or more complex snarl. The cartoon depicts the case of an heterozygous insertion and an homozygous deletion. The algorithm is described in more details in [Methods](#toil-vg-call).
b) Simulation experiment. Each subplot shows a comparison of genotyping accuracy for four SV calling methods. Results are separated between types of variation (insertions, deletions, and inversions). The experiments were also repeated with small random errors introduced to the VCF to simulate breakpoint uncertainty. For each experiment, the y-axis shows the maximum F1 across different minimum quality thresholds.
[//]: # (Remove from figure adn add line: SVTyper cannot genotype insertions, hence the missing line in the top panels.)
](images/panel1.png){#fig:1}

### Simulated dataset

As a proof of concept, we simulated genomes and different types of SVs with a size distribution matching real SVs[@tag:hgsvc].
We compared vg against SVTyper, Delly, and BayesTyper across different levels of sequencing depth.
We also added some errors (1-10bp) to the location of the breakpoints to investigate their effect on genotyping accuracy (see [Methods](#simulation-experiment)).
The results are shown in Figure {@fig:1}b.

When using the correct breakpoints, vg tied with Delly as the best genotyper for deletions, and with BayesTyper as the best genotyper for insertions.
For inversions, vg was the second best genotyper after BayesTyper.
The differences between the methods were the most visible at lower sequencing depth. 
In the presence of 1-10 bp errors in the breakpoint locations, the performance of Delly and BayesTyper dropped significantly (Figure {@fig:1}b).
The dramatic drop for BayesTyper can be explained by its k-mer-based approach that requires precise breakpoints.
In contrast, vg was only slightly affected by the presence of errors.
For vg, the F1 scores for all SV types decreased no more than 0.07.
Overall, these results show that vg is capable of genotyping SVs and is robust to breakpoint inaccuracies in the input VCF.

### HGSVC dataset

72,485 structural variants from The Human Genome Structural Variation Consortium (HGSVC) were used to benchmark the genotyping performance of vg against the three other SV genotyping methods.
This high-quality SV catalog was generated from three samples using a consensus from different sequencing, phasing, and variant calling technologies[@tag:hgsvc]. 
The three individual samples represent different human populations: Han Chinese (HG00514), Puerto-Rican (HG00733), and Yoruban Nigerian (NA19240).
We used these SVs to construct a graph with vg and as input for the other genotypers.
Using short sequencing reads, the SVs were genotyped and compared with the genotypes in the original catalog (see [Methods](#hgsvc-analysis)).

First we compared the methods using simulated reads for HG00514.
This represents the ideal situation where the SV catalog exactly matches the SVs supported by the reads.
While vg outperformed Delly and SVTyper, BayesTyper showed the best F1 score and precision-recall trade-off (Figures {@fig:2} and {@fig:hgsvc-sim-geno}, Table {@tbl:hgsvc}).
When restricting the comparisons to regions not identified as tandem repeats or segmental duplications, the genotyping predictions were significantly better for all methods, with vg almost as good as BayesTyper on deletions (F1 of 0.944 vs 0.955).
We observed similar results when evaluating the presence of an SV call instead of the exact genotype (Figures {@fig:2} and {@fig:hgsvc-sim}).
Overall, both graph-based methods, vg and BayesTyper, outperformed the other two methods tested.

![**Structural variants from the HGSVC and Genome in a Bottle datasets**. 
HGSVC: Simulated and real reads were used to genotype SVs and compared with the high-quality calls from Chaisson et al.[@tag:hgsvc].
Reads were simulated from the HG00514 individual.
Using real reads, the three HG00514, HG00733, and NA19240 individuals were tested.
GiaB: Real reads from the HG002 individual were used to genotype SVs and compared with the high-quality calls from the Genome in a Bottle consortium[@doi:10.1038/sdata.2016.25;@doi:10.1101/281006].
Maximum F1 score for each method (color), across the whole genome or focusing on non-repeat regions (x-axis). 
We evaluated the ability to predict the presence of an SV (transparent bars) and the exact genotype (solid bars).
Results are separated across panels by variant type: insertions and deletions.
SVTyper cannot genotype insertions, hence the missing bars in the top panels.
](images/hgsvc-giab-best-f1.png){#fig:2}

We then repeated the analysis using real Illumina reads from the three HGSVC samples to benchmark the methods on a more realistic experiment.
Here, vg clearly outperformed other approaches (Figures {@fig:2} and {@fig:hgsvc-real-geno}).
In non-repeat regions and across the whole genome, the F1 scores and precision-recall AUC were higher for vg compared to other methods.
For example, for deletions in non-repeat regions, the F1 score for vg was 0.801 while the second best method, Delly, had a F1 score of 0.692.
We observed similar results when evaluating the presence of an SV call instead of the exact genotype (Figures {@fig:2} and {@fig:hgsvc-real}).
Figure {@fig:hgsvc-ex} shows examples of an exonic deletion and an exonic insertion that were correctly genotyped by vg but not by the other methods.

![**Exonic SVs in the HGSVC dataset correctly genotyped by vg**. 
Visualizations of the HGSVC graph as augmented by reads aligned by vg at two loci exonic loci harboring SVs.
(A) 51 bp homozygous deletion in the last exon of the LONRF2 gene.
(B) 114 bp homozygous insertion in a short tandem repeat region overlapping the first exon of the MED13L gene, a gene predicted to be loss of function intolerant.
At bottom of each rendering, a horizontal black line represents the topologically sorted nodes of the graph.
Black rectangles represent edges found in the graph.
Above this rendering of the topology, the reference path from GRCh38 is shown (either in green or blue).
Red and blue bars represent reads mapped to the graph.
Thin lines in the reference path and read mappings highlight relative gaps (either insertions or deletions) against the full graph.
The vg read mappings show consistent coverage even over these SVs.
](images/panel7.png){#fig:hgsvc-ex}


### Other long-read datasets

#### Genome in a Bottle Consortium

The Genome in a Bottle (GiaB) consortium is currently producing a high-quality SV catalog for an Ashkenazim individual (HG002)[@doi:10.1038/sdata.2016.25;@doi:10.1101/281006].
Dozens of SV callers operating on datasets from short, long, and linked reads were used to produce this set of SVs.
We evaluated the SV genotyping methods on this sample as well using the GIAB VCF, which also contains parental calls (HG003 and HG004), all totalling 30,224 SVs.
vg performed similarly on this dataset as on the HGSVC dataset, with a F1 score of 0.75 for both insertions and deletions in non-repeat regions (Figures {@fig:2}, {@fig:giab-geno} and {@fig:giab}, and Table {@tbl:giab}).
As before, other methods produced lower F1 scores in most cases, although Delly and BayesTyper predicted better genotypes for deletions in non-repeat regions.

#### SMRT-SV v2 catalog and training data [@tag:audano2019]

A recent study by Audano et al. generated a catalog of 97,368 SVs (referred as SVPOP below) using long-read sequencing across 15 individuals[@tag:audano2019].
These variants were then genotyped from short reads across 440 individuals using the SMRT-SV v2 genotyper, a machine learning-based tool implemented for that study.
The SMRT-SV v2 genotyper was trained on a pseudo-diploid genome constructed from high quality assemblies of two haploid cell lines (CHM1 and CHM13) and a single negative control (NA19240).
We first used vg to genotype the SVs in this two-sample training dataset using 30X coverage reads, and compared the results with the SMRT-SV v2 genotyper.
vg was systematically better at predicting the presence of an SV for both SV types, but SMRT-SV v2 produced better genotypes for deletions (see Figures {@fig:chmpd-svpop}, {@fig:chmpd-geno} and {@fig:chmpd}, and Table {@tbl:chmpd}). 
To compare vg and SMRT-SV v2, we then genotyped SVs from the entire SVPOP catalog with both methods, using the read data from the three HGSVC samples described above.
Given that the the SVPOP catalog contains these three samples, we once again evaluated accuracy by using the long-read genotypes as a baseline of comparison.

Compared to SMRT-SV v2, vg had a better precision-recall curve and a higher F1 for both insertions and deletions (SVPOP in Figures {@fig:chmpd-svpop} and {@fig:svpop}, and Table {@tbl:svpop}).
Of note, SMRT-SV v2 produces *no-calls* in regions where the read coverage is too low, and we observed that its recall increased when filtering these regions out the input set.
Interestingly, vg performed well even in regions where SMRT-SV v2 produced *no-calls* (Figure {@fig:svpop-regions} and Table {@tbl:svpop-regions}).
Audano et al. discovered 217 sequence-resolved inversions using long reads, which we attempted to genotype.
vg correctly predicted the presence of around 14% of the inversions present in the three samples (Table {@tbl:svpop}).
Inversions are often complex, harboring additional variation that makes their characterization and genotyping challenging.

![**Structural variants from SMRT-SV v2 [@tag:audano2019]**.
The pseudo-diploid genome built from two CHM cell lines and one negative control sample was originally used to train SMRT-SV v2 in Audano et al.[@tag:audano2019].
It contains 16,180 SVs.
The SVPOP panel shows the combined results for the HG00514, HG00733, and NA19240 individuals, three of the 15 individuals used to generate the high-quality SV catalog in Audano et al. [@tag:audano2019].
Here, we report the maximum F1 score (y-axis) for each method (color), across the whole genome or focusing on non-repeat regions (x-axis). 
We evaluated the ability to predict the presence of an SV (transparent bars) and the exact genotype (solid bars).
Genotype information is not available in the SVPOP catalog hence genotyping performance could not be evaluated.
](images/chmpd-svpop-best-f1.png){#fig:chmpd-svpop}

### Graphs from alignment of *de novo* assemblies

We can construct variation graphs directly from whole genome alignments of multiple *de novo* assemblies[@tag:vgnbt].
This bypasses the need for generating an explicit variant catalog relative to a linear reference, which could be a source of error due to the reference bias inherent in read mapping and variant calling.
Genome alignments from graph-based software such as Cactus [@doi:10.1101/gr.123356.111] can contain complex structural variation that is extremely difficult to represent, let alone call, outside of a graph, but which is nevertheless representative of the actual genomic variation between the aligned assemblies.
We sought to establish if graphs built in this fashion provide advantages for SV genotyping.

To do so, we analyzed public sequencing datasets for 12 yeast strains from two related clades (*S. cerevisiae* and *S. paradoxus*) [@doi:10.1038/ng.3847].
We compared genotyping results using two different types of genome graphs.
The graphs were constructed using 5 of the 12 strains.
*S.c. S288C* was used as the reference strain, and we selected two other strains from each of the two clades (see [Methods](#yeast-graph-analysis)).
The first graph (called *VCF graph* below) was created from the linear reference genome of the *S.c. S288C* strain and a set of SVs relative to this reference strain in VCF format identified from the other four assemblies by three methods: Assemblytics [@doi:10.1093/bioinformatics/btw369], AsmVar [@doi:10.1186/s13742-015-0103-4] and paftools [@doi:10.1093/bioinformatics/bty191].
The second graph (called *cactus graph* below) was derived from a multiple genome alignment of the five strains using Cactus [@doi:10.1101/gr.123356.111].
The *VCF graph* is mostly linear and highly dependent on the reference genome.
In contrast, the *cactus graph* is structurally complex and relatively free of reference bias.

First, we tested our hypothesis that the *cactus graph* has higher mappability due to its better representation of sequence diversity among the yeast strains (see [Supplementary Information](#mappability-comparison-between-yeast-graphs)).
Generally, more reads mapped to the *cactus graph* with high identity (Figure {@fig:panel3}a) and high mapping quality (Figure {@fig:panel3}b) than to the *VCF graph*.

Next, we compared the SV genotyping performance of both graphs.
We mapped short reads from the 11 non-reference strains to both graphs and called variants for each strain using the vg toolkit's variant calling module (see [Methods](#toil-vg-call)).
There is no gold standard call set for these samples, so we used an indirect measure of SV calling accuracy.
We evaluated each call set based on the alignment of reads to a *sample graph* constructed from the call set (see [Methods](#calling-and-genotyping-of-svs)).
If a given call set is correct, we expect that reads from the same sample will be mapped with high identity and confidence to the corresponding sample graph.
Therefore, we compared the average percent identity and mapping quality of the short reads on each sample graph (Figures {@fig:4}a and b).
Similar to the mappability results, the *cactus graph* clearly outperformed the *VCF graph* for strains in the *S. paradoxus* clade and performed slightly better for strains in the *S. cerevisiae* clade.
While the higher percent identity shows that the *cactus graph* represents the reads better (Figures {@fig:4}a), the higher mapping quality confirms that this did not come at the cost of added ambiguity or a more complex graph (Figures {@fig:4}b).
[//]: # (Think it might be worth mentioning the difference in non-SV origin somewhere is this section. Assemblies for cactus graphs, and vg call for vcf graphs. I know that some of it is already mentioned in the supplementary, but I think that it is also relevant here in order to understand the results in figure 5 better. As you also mentions in one of the issues, some of the difference could be explained by less mapped reads to the vcf graphs due to lacking non-SVs, resulting fewer called variants and thus a less complete sample graph.)
Our results did not show a substantial difference between strains included in the graph and those that were excluded.
This suggests that two strains from each clade as well as the reference strain are sufficient to capture most of the genetic variation among all the strains.
For a direct comparison, see Figure {@fig:panel6} which shows results of the same experiment on graphs generated from all 12 strains.

![**SV genotyping comparison.**
Short reads from all 11 non-reference yeast strains were used to genotype SVs contained in both graphs. 
Subsequently, sample graphs were generated from the resulting SV callsets. 
[//]: # (Are SVs only used to construct the sample graphs? The methods section does not mention any filtering.)
The short reads were aligned to the sample graphs and the quality of the alignments was used to ascertain genotyping performance.
More accurate genotypes should result in reference graphs that have mappings with high identity and confidence for a greater proportion of the reads.
a) Average mapping identity of short reads aligned to the sample graphs derived from *cactus graph* (y-axis) and *VCF graph* (x-axis).
b) Average mapping quality of short reads aligned to the sample graphs derived from *cactus graph* (y-axis) and *VCF graph* (x-axis).
Colors and shapes represent the 11 non-reference strains and two clades, respectively. 
Transparency indicates whether the strain was used to construct the graphs.
](images/panel4.png){#fig:4}

