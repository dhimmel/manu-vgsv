## Results

### Structural variation in vg

A variation graph encodes DNA sequence in its nodes.
Such graphs are bidirected, in that we distinguish between edges incident on the starts of nodes from those incident on their ends.
A path in such a graph is an  ordered list of nodes where each is associated with an orientation.
If a path walks from, for example, node A in the forward orientation to node B in the reverse orientation, then an edge must exist from the end of node A to the end of node B.
Concatenating the sequences on each node in the path, taking the reverse complement when the node is visited in reverse orientation, produces a string, and it is in this way that haploid sequences are embedded in the graph.
Variation between sequences shows up as bubbles in the graph [@tag:snarls].
Figure {@fig:1}a shows how a graph with a SNP and an indel can be extended to contain more complex SVs.

The vg toolkit provides open source tools for constructing variation graphs and mapping reads to them[@tag:vgnbt].
We used it to implement a simple SV genotyping pipeline.
Reads are mapped to the graph and used to compute the read support for each node and edge.
Sites of variation are then identified using the snarl decomposition as described in [@tag:snarls].
For each site, the two most supported paths (haplotypes) are determined, and their relative supports used to produce a genotype at that site.
The pipeline is described in more detail in [Methods](#simulation-experiment).

As a proof-of-concept we simulated genomes and different types of SVs with a size distribution matching real SVs[@tag:hgsvc].
We compared vg against SVTyper, Delly and BayesTyper across different levels of sequencing depth.
Some errors were also added at the breakpoints to investigate their effect on genotyping accuracy (see [Methods](#simulation-experiment)).
The results are shown in Figure {@fig:1}b.
When using the correct breakpoints, vg tied with Delly as the best genotyper for deletions, and with BayesTyper as the best genotyper for insertions.
For inversions, vg was the second best genotyper after BayesTyper.
The differences between the methods were the most visible at lower sequencing depth. 
In the presence of 1-10 bp errors in the breakpoint locations, the performance of Delly and BayesTyper dropped significantly (Figure {@fig:1}b).
The dramatic drop for BayesTyper can be explained by its k-mer-based approach that requires precise breakpoints.
In contrast, vg was only slightly affected by the presence of errors.
For vg, the F1 scores for all SV types decreased no more than 0.07.
Overall, these results show that vg is capable of genotyping SVs and is robust to breakpoint inaccuracies in the input VCF.

![**Structural variation in vg.** 
a) Adding large insertions, deletions and inversions in a variation graph. 
b) Simulation experiment. For each experiment (method, depth and input VCF with/without breakpoint errors), the y-axis shows the maximum F1 across different minimum quality thresholds.
](images/panel1.png){#fig:1}

### HGSVC dataset

Structural variants from The Human Genome Structural Variation Consortium (HGSVC) were used to benchmark the genotyping performance of vg against 3 other SV genotyping methods
This high-quality SV catalog was generated from three samples using a consensus from different sequencing, phasing and variant calling technologies[@tag:hgsvc]. 
The three samples come from different human populations: a Han Chinese individual (HG00514), a Puerto-Rican individual (HG00733), and a Yoruban Nigerian individual (NA19240).
These SVs were used to construct a graph with vg and as input for the other genotypers.
SVs were genotyped from short reads and compared with the genotypes in the original catalog (see [Methods](#hgsvc-analysis)).

First, by simulating reads for HG00514, we compared the different methods in the ideal error-free situation where the SV catalog matches exactly the SVs supported by the reads.
While vg outperformed Delly and SVTyper, BayesTyper showed the best F1 score and precision-recall trade-off (Figures {@fig:2} and {@fig:hgsvc-sim-geno}, Table {@tbl:hgsvc}).
When restricting the comparisons to regions not identified as tandem repeats or segmental duplications, the genotyping predictions were significantly better for all methods, with vg almost as good as BayesTyper on deletions (F1 of 0.944 vs 0.955).
We observed similar results when evaluating the presence of an SV call instead of the exact genotype (Figures {@fig:2} and {@fig:hgsvc-sim}).
Overall, both graph-based methods, vg and BayesTyper, outperformed the other two methods tested.

We then repeated the analysis using real Illumina reads from HG00514 to benchmark the methods on a more realistic experiment.
Here vg clearly outperformed other approaches, most likely because of its graph-based strategy and robustness to errors in the SV catalog (Figures {@fig:2} and  {@fig:hgsvc-real-geno}).
In non-repeat regions and across the whole genome, the F1 scores and precision-recall curves were higher for vg compared to other methods. 
For example, for deletions in non-repeat regions, the F1 score for vg was 0.801 while the second best method, Delly, had a F1 score of 0.692.
We observed similar results when evaluating the presence of an SV call instead of the exact genotype (Figures {@fig:2} and {@fig:hgsvc-real}).

![**Structural variants from the HGSVC and Genome in a Bottle datasets**. 
HGSVC: Simulated and real reads were used to genotype SVs and compared with the high-quality calls from Chaisson et al.[@tag:hgsvc].
Reads were simulated from the HG00514 individual.
Using real reads, the three HG00514, HG00733, and NA19240 individuals were tested.
GiaB: Real reads from the HG002 individual were used to genotype SVs and compared with the high-quality calls from the Genome in a Bottle consortium[@doi:10.1038/sdata.2016.25;@doi:10.1101/281006].
Maximum F1 score for each method (color), across the whole genome or focusing on non-repeat regions (x-axis). 
We evaluated the ability to predict the presence of an SV (transparent bars) and the exact genotype (solid bars).
SVTyper cannot genotype insertions hence the absent bars in the top panel.
](images/hgsvc-giab-best-f1.png){#fig:2}

### Other long-read datasets

The Genome in a Bottle (GiaB) consortium is currently producing a high-quality SV catalog for a Ashkenazim individual (HG002)[@doi:10.1038/sdata.2016.25;@doi:10.1101/281006].
Dozens of SV callers and datasets from short, long and linked reads were used to produce this set of SVs.
vg performed similarly on this dataset as on the HGSVC dataset, with a F1 score of 0.75 for both insertions and deletions in non-repeat regions (Figures {@fig:2}, {@fig:giab-geno} and {@fig:giab}, and Table {@tbl:giab}).
As before, other methods produced lower F1 scores in most cases, although Delly and BayesTyper predicted better genotypes for deletions in non-repeat regions.
Because this catalog contained SVs for only one individual, this benchmark is most relevant when comparing the recall, which is what contributes most to the F1 score differences (Figure {@fig:giab}).

A recent study by Audano et al. generated an SV catalog using long-read sequencing across 15 individuals[@tag:audano2019].
These variants were then genotyped from short reads across 440 individuals using SMRT-SV2, a machine-learning genotyper implemented for this study.
SMRT-SV2 was trained on a pseudo-diploid genome constructed from high quality assemblies of two haploid cell lines.
We first called SVs in this dataset, using the same SV catalog and short read dataset.
vg was systematically better at predicting the presence of an SV for both SV types, but SMRT-SV2 produced better genotypes for deletions (see Figures {@fig:chmpd-svpop}, {@fig:chmpd-geno} and {@fig:chmpd}, and Table {@tbl:chmpd}). 
Using publicly available Illumina reads, we then genotyped SVs in 3 of the 15 individuals that were used for discovery in Audano et al.[@tag:audano2019].
Compared to SMRT-SV2, vg had a better precision-recall curve and a higher F1 for both insertions and deletions (SVPOP in Figures {@fig:chmpd-svpop} and {@fig:svpop}, and Table {@tbl:svpop}).
Of note, SMRT-SV2 produces *no-calls* in regions where the read coverage is too low, and we observed that its recall increased when filtering these regions out the input set.
Interestingly, vg performed well even in regions where SMRT-SV2 produced *no-calls* (Figure {@fig:svpop-regions} and Table {@tbl:svpop-regions}).
Finally, Audano et al. identified 217 sequence-resolved inversions.
vg correctly predicted the presence of around 14% of the inversions present in the three samples (Table {@tbl:svpop}).
Inversions are often complex, harboring additional variation that makes their characterization and genotyping challenging.

![**Structural variants from Audano et al.[@tag:audano2019]**.
The pseudo-diploid genome built from CHM cell lines was originally used to train SMRT-SV2 in Audano et al.[@tag:audano2019].
The SVPOP panel shows the combined results for the HG5014, HG00733 and NA19240 individuals, 3 of the 15 individuals used to generate the high-quality SV catalog in Audano et al.[@tag:audano2019].
Maximum F1 score for each method (color), across the whole genome or focusing on non-repeat regions (x-axis). 
We evaluated the ability to predict the presence of an SV (transparent bars) and the exact genotype (solid bars).
Genotype information is not available in the SVPOP catalog hence genotyping performance could not be evaluated.
](images/chmpd-svpop-best-f1.png){#fig:chmpd-svpop}


### Breakpoint fine-tuning

*(Maybe better in discussion.)*

In addition to genotyping, vg can use an augmentation step to modify the graph based on the read alignment and discover novel variants.
On the simulated SVs from Figure {@fig:1}b, this approach was able to correct many of the 1-10 bp breakpoint errors that were added to the input VCF.
The breakpoints were accurately fine-tuned for 93.8% of the insertions (Figure {@fig:simerror-bkpt}a and Table {@tbl:simerror-bkpt}).
For deletions, 78.1% of the variants were corrected when only one breakpoint had an error.
In situations where both breakpoints of the deletions were incorrect, only 18.6% were corrected through graph augmentation, and only when the amount of error was small (Figure {@fig:simerror-bkpt}b).
The breakpoints of less than 20% of the inversions could be corrected.
Across all SV types, the size of the variant didn't affect the ability to fine-tune the breakpoints through graph augmentation (Figure {@fig:simerror-bkpt}c).


### Graphs from alignment of de novo assemblies

Genome graphs can be constructed directly from multiple sequence alignments of de novo assemblies[@tag:vgnbt].
This bypasses the need for going through a variant caller which could be a source of error (for example, from reference bias).
Furthermore, genome alignments from software such as Cactus [@doi:10.1101/gr.123356.111] can contain complex structural variation that is extremely difficult to represent, let alone call, outside of a graph.
We therefore investigated whether genome graphs derived from de-novo assembly alignments yield advantages for SV genotyping.
To this end, we analyzed public sequencing datasets for 12 yeast strains from two related clades (*S. cerevisiae* and *S. paradoxus*) [@doi:10.1038/ng.3847].

By generating genome graphs from only five of the strains we could measure how well we can assay variation in the full set of 12 strains using only limited information from a small subset of strains.
We generated and compared two different types of genome graphs.
The first graph type (in the following called *VCF graph*) was created from a linear reference genome of the *S.c. S288C* strain and a set of SVs relative to this reference strain in VCF format.
We compiled the SV set using the output of three methods for SV detection from genome assemblies: Assemblytics [@doi:10.1093/bioinformatics/btw369], AsmVar [@doi:10.1186/s13742-015-0103-4] and paftools [@doi:10.1093/bioinformatics/bty191].
All three methods were run to detect SVs between the reference strain and each of the other strains.
Merging the results from the three methods and four of the eleven strains provided us with a representative set of SVs occurring in the two yeast clades that we could use to construct the *VCF graph*.
The second graph (in the following called *cactus graph*) was derived from a multiple genome alignment of the five strains using our Cactus tool [@doi:10.1101/gr.123356.111].
The *VCF graph* is mainly linear and highly dependent on the reference genome.
In contrast, the *cactus graph* is structurally complex and free of reference bias.

![**Mapping comparison.** 
Short reads from all 12 yeast strains were aligned to both graphs. 
The fraction of reads mapped to the cactus graph (y-axis) and the VCF graph (x-axis) are compared.
a) Stratified by mapping quality threshold.
b) Stratified by percent identity threshold.
Colors and shapes represent the 12 strains and two clades, respectively. 
Transparency indicates whether the strain was included or excluded in the graphs.
](images/panel3.png){#fig:3}

First, we tested our hypothesis that the *cactus graph* has higher mappability due to its better representation of sequence diversity among the yeast strains.
Figure {@fig:3}a shows the fraction of Illumina reads from the 12 strains that was mapped with a mapping quality above a certain threshold to the *cactus graph* and to the *VCF graph*.
Generally, more reads were mapped to the *cactus graph* than to the *VCF graph* regardless of the chosen mapping quality threshold.
Only for the reference strain *S.c. S288C*, the *VCF graph* exhibited slightly better mappability.
This suggests that the improvement in mappability is not driven by the higher sequence content in the *cactus graph* alone (15.4 Mb compared to 12.4 Mb in the *VCF graph*).
Instead, our measurements suggest that the genetic distance to the reference strain correlates with the better mapping on the *cactus graph* over the *VCF graph*.
Consequently, the benefit of using the *cactus graph* is largest for strains in the *S. paradoxus* clade and smaller for reads from strains in the *S. cerevisiae* clade.

We observed a similar trend when exploring the mapping identity of the short reads on the graphs (see Figure {@fig:3}b).
For strains in the *S. paradoxus* clade, the *cactus graph* resulted in substantially more mappings with high percent identity than the *VCF graph*.
With strains in the *S. cerevisiae* clade, the difference was smaller, at least for a percent identity threshold up to 90%.
When comparing read fractions with perfect identity (i.e. percent identity threshold = 100%), the *cactus graph* clearly outperforms the *VCF graph* on 11 out of 12 samples, the exception again being the reference strain *S.c. S288C*. One reason behind this is that the VCF graph only contains SVs but no SNPs or Indels. The cactus graph which is constructed directly from de novo assemblies consequently captures the genetic makeup of each strain more comprehensively and accurately.

Interestingly, our measurements did not show a substantial difference between strains included in the graph and excluded strains. The results suggest that two strains from each clade as well as the reference strain are sufficient to capture most of the genetic variation among all the strains. Only the number of alignments with perfect identity is substantially lower for the strains that were not included in the creation of the graphs (see Figure {@fig:3}b). For a direct comparison, see Figure {@fig:panel5} which shows results of the same experiment on graphs generated from all 12 strains.

Next, we compared the SV genotype performance of both graphs.
We mapped short reads from the 11 non-reference strains to both graphs and called variants for each strain using the vg toolkit's variant calling module (see [Methods](#toil-vg-call)).
In the absence of a gold standard, we evaluated each callset based on the alignment of reads to a *sample graph* constructed from the callset (see [Methods](#calling-and-genotyping-of-svs)).
If a given callset is correct, we expect that reads from the same sample will be mapped confidently and with high identity to the corresponding sample graph.
Therefore, we compared the average mapping quality and percent identity of the short reads on each sample graph (see Figures {@fig:4}a and b).
Similar to the results of our mapping analysis above, the *cactus graph* clearly outperformed the *VCF graph* for strains in the *S. paradoxus* clade and performed slightly better for strains in the *S. cerevisiae* clade.
Again, our measurements did not show a large difference between strains included in the graph and those that were excluded. For a direct comparison, see Figure {@fig:panel6} which shows results of the same experiment on graphs generated from all 12 strains.

![**SV genotyping comparison.**
Short reads from all 11 non-reference yeast strains were used to genotype SVs contained in both graphs. 
Subsequently, sample graphs were generated from the resulting SV callsets. 
The short reads were again aligned to the sample graphs and the quality of the alignments was used to ascertain genotyping performance.
a) Average mapping quality of short reads aligned to the sample graphs derived from *cactus graph* (y-axis) and *VCF graph* (x-axis).
b) Average mapping identity of short reads aligned to the sample graphs derived from *cactus graph* (y-axis) and *VCF graph* (x-axis). 
Colors and shapes represent the 11 non-reference strains and two clades, respectively. 
Transparency indicates whether the strain was included or excluded in the graphs.
](images/panel4.png){#fig:4}

